from .metrics import MetricCalculator
from .bedrock_integration import BedrockEvaluator
# from .ragas_evaluator import RagasEvaluator
from .context_evaluator import ContextEvaluator
import logging

class LLMEvaluator:
    def __init__(self, bedrock_region='us-east-1'):
        """
        Initialize LLM Evaluator with metrics and Bedrock integration
        
        :param bedrock_region: AWS region for Bedrock service
        """
        self.metric_calculator = MetricCalculator()
        self.bedrock_evaluator = BedrockEvaluator(region_name=bedrock_region)
        # self.ragas_evaluator = RagasEvaluator()
        self.context_evaluator = ContextEvaluator()
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def evaluate(self, generated_text, reference_text=None, metrics=None, bedrock_model=None,trace=False):
        """
        Comprehensive evaluation of generated text
        
        :param generated_text: Text generated by the model
        :param reference_text: Ground truth text (optional)
        :param metrics: List of metrics to calculate
        :param bedrock_model: Bedrock model for prompt-based evaluation
        :return: Evaluation results
        """
        # Default metrics if not specified
        if metrics is None:
            metrics = ['rouge', 'bleu', 'semantic_similarity','meteor_score']
        
        # Initialize results dictionary
        results = {
            'generated_text': generated_text
        }

        # Calculate specified metrics
        try:
            for metric in metrics:
                if reference_text:
                    if metric == 'rouge':
                        results['rouge'] = self.metric_calculator.calculate_rouge(generated_text, reference_text)
                    elif metric == 'bleu':
                        results['bleu'] = self.metric_calculator.calculate_bleu(generated_text, reference_text)
                    elif metric == 'semantic_similarity':
                        results['semantic_similarity'] = self.metric_calculator.semantic_similarity(generated_text, reference_text)
                    elif metric == 'meteor_score':
                        results['meteor_score'] = self.metric_calculator.calculate_meteor_score(generated_text, reference_text)
                else:
                    self.logger.warning(f"Skipping {metric} as reference text is not provided")

            # Perform Bedrock-based evaluation if a model is specified
            if bedrock_model:
                bedrock_eval = self.bedrock_evaluator.evaluate_with_prompt(
                    generated_text, 
                    model_name=bedrock_model
                )
                results['model_evaluation'] = bedrock_eval['evaluation']
                results['model'] = bedrock_eval['model']
                if trace:
                    results['metadata'] = bedrock_eval['metadata']
                

        except Exception as e:
            self.logger.error(f"Error during evaluation: {e}")
            results['error'] = str(e)

        return results

    def custom_prompt_evaluation(self, generated_text, custom_prompt, bedrock_model='claude-3-Opus'):
        """
        Perform custom prompt-based evaluation using Bedrock
        
        :param generated_text: Text to evaluate
        :param custom_prompt: Custom evaluation prompt
        :param bedrock_model: Bedrock model to use
        :return: Evaluation results
        """
        try:
            return self.bedrock_evaluator.evaluate_with_prompt(
                generated_text, 
                evaluation_prompt=custom_prompt,
                model_name=bedrock_model
            )
        except Exception as e:
            self.logger.error(f"Custom prompt evaluation error: {e}")
            return {'error': str(e)}

    def evaluate_rag(self, questions=None, answers=None, contexts=None, metrics=None, ground_truths=None):
        """
        Evaluate RAG outputs using RAGAS metrics
        
        :param questions: List of questions
        :param answers: List of generated answers
        :param contexts: List of contexts used for generation
        :param metrics: List of RAGAS metrics to use
        :param ground_truths: Optional list of ground truth answers
        :return: RAGAS evaluation results
        """
        return "work in progress"
        try:
            return self.ragas_evaluator.evaluate_rag(
                questions=questions,
                answers=answers,
                contexts=contexts,
                metrics=metrics,
                ground_truths=ground_truths
            )
        except Exception as e:
            self.logger.error(f"RAG evaluation error: {e}")
            return {'error': str(e)}

    def evaluate_context(self, question, context, answer=None, prompt_types=None, bedrock_model='claude-3-Opus'):
        """
        Evaluate context using prompt-based evaluation and Bedrock
        
        :param question: Question being asked
        :param context: Context provided
        :param answer: Generated answer (optional)
        :param prompt_types: List of prompt types to use
        :param bedrock_model: Bedrock model to use for evaluation
        :return: Evaluation results with numerical scores
        """
        try:
            # Get evaluation prompts
            evaluation_prompts = self.context_evaluator.evaluate_context(
                question=question,
                context=context,
                answer=answer,
                prompt_types=prompt_types
            )
            
            # Evaluate using Bedrock and extract scores
            results = {}
            scores = {}

            bedrock_model_id = self.bedrock_evaluator.select_model(bedrock_model)
            
            for prompt_type, prompt in evaluation_prompts.items():

                # Get evaluation from Bedrock
                evaluation = self.bedrock_evaluator.model_invoke(
                    modelId=bedrock_model_id,
                    prompt=prompt
                )
                

                # Extract scores from evaluation
                score_data = self.context_evaluator.extract_scores(evaluation[0])
                results[prompt_type] = score_data
                
                # Add scores to aggregated scores dictionary
                if 'scores' in score_data:
                    scores[prompt_type] = score_data['scores']
            
            return {
                'detailed_results': results,
                'scores': scores,
                'prompts_used': list(evaluation_prompts.keys())
            }
            
        except Exception as e:
            self.logger.error(f"Context evaluation error: {e}")
            return {'error': str(e)}

    def add_custom_context_prompt(self, prompt_template):
        """
        Add a custom context evaluation prompt
        
        :param prompt_template: Custom prompt template
        :return: Name of the created prompt
        """
        return self.context_evaluator.create_custom_prompt(prompt_template)

    def get_available_context_prompts(self):
        """
        Get list of available context evaluation prompts
        
        :return: List of prompt names
        """
        return self.context_evaluator.get_available_prompts()
